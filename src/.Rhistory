# Log transform
#log.dat <- log(dat)
# Apply PCA
dat.pca <- prcomp(dat,
center = TRUE,
scale. = TRUE)
# This will pipe out put to file
sink (paste(resultPath,pcaTitle, '.txt', sep = ''), append = TRUE, split = FALSE)
print(summary(dat.pca))
print(dat.pca)
# Return output to console
sink()
# Plot PCA
jpeg(paste(resultPath,pcaTitle,".jpg", sep = ""))
plot(dat.pca, type = "l", xaxt = 'n', yaxt = 'n', main = pcaTitle)
axis(1,at = 1:7 names)
dev.off()
}
source('~/Documents/MarketTrendPrediction/src/Model-Functions.r')
getpca <- function (dat, resultPath, pcaTitle,names) {
# Principle Component Analysis
# Log transform
#log.dat <- log(dat)
# Apply PCA
dat.pca <- prcomp(dat,
center = TRUE,
scale. = TRUE)
# This will pipe out put to file
sink (paste(resultPath,pcaTitle, '.txt', sep = ''), append = TRUE, split = FALSE)
print(summary(dat.pca))
print(dat.pca)
# Return output to console
sink()
# Plot PCA
jpeg(paste(resultPath,pcaTitle,".jpg", sep = ""))
plot(dat.pca, type = "l", xaxt = 'n', yaxt = 'n', main = pcaTitle)
axis(1,at = 1:7, names)
dev.off()
}
getpca(prepdata(housing_dat,names, normalization = "mm"), result_path, "PCA-Monroe-County-1", names)
source('~/Documents/MarketTrendPrediction/src/Model-Functions.r')
getpca <- function (dat, resultPath, pcaTitle,names) {
# Principle Component Analysis
# Log transform
#log.dat <- log(dat)
# Apply PCA
dat.pca <- prcomp(dat,
center = TRUE,
scale. = TRUE)
# This will pipe out put to file
sink (paste(resultPath,pcaTitle, '.txt', sep = ''), append = TRUE, split = FALSE)
print(summary(dat.pca))
print(dat.pca)
# Return output to console
sink()
# Plot PCA
jpeg(paste(resultPath,pcaTitle,".jpg", sep = ""))
par(xaxt = 'n')
plot(dat.pca, type = "l", main = pcaTitle)
axis(1,at = 1:7, names)
dev.off()
}
getpca(prepdata(housing_dat,names, normalization = "mm"), result_path, "PCA-Monroe-County-1", names)
plot(median_sold_price)
plot(median_sold_price)
dev.off()
plot(median_sold_price)
par(xaxt = 'n')
plot(median_sold_price)
plot(median_sold_price)
plot.new()
par(xaxt = "n")
plot(median_sold_price)
axis(1,at = 1:7, names)
plot.new()
plot(median_sold_price)
axis(1,at = 1:7, names)
dev.off()
plot(median_sold_price)
axis(1,at = 1:7, names)
dev.off()
par(xaxt = "n")
plot(median_sold_price)
axis(1,at = 1:7, names)
dev.off()
plot(median_sold_price, xaxt = names)
dev.off()
axis(1,at = 1:7, names)
plot(median_sold_price, xaxt = 'n')
axis(1,at = 1:7, names)
par(xaxt = 'y')
legend("topright", inset=.05, title="Number of Cylinders",
names, fill=terrain.colors(3), horiz=TRUE)
legend("topright", inset=.05, title="Number of Cylinders",
names, fill=terrain.colors(3), horiz=FALSE)
legend("topright", inset=.05, title="Number of Cylinders",
names, fill=terrain.colors(7), horiz=FALSE)
?legend
legend("topright", inset=.1, title="Number of Cylinders",
names, fill=terrain.colors(3), horiz=FALSE)
dev.off()
source('~/Documents/MarketTrendPrediction/src/Model-Functions.r')
# file: Model-Functions.r
# Contributers: William Hammond
#               Harry Longwell
# Created: 11/19/2015
# Modified: 11/22/2015
library(nnet)
# Function that computes and plots PCA for a given data set.
# Input:
#       dat = data set
#       resultPath = Path to results folder
#       pcaTitle = title to plot
getpca <- function (dat, resultPath, pcaTitle,names) {
# Principle Component Analysis
# Log transform
#log.dat <- log(dat)
# Apply PCA
dat.pca <- prcomp(dat,
center = TRUE,
scale. = TRUE)
# This will pipe out put to file
sink (paste(resultPath,pcaTitle, '.txt', sep = ''), append = TRUE, split = FALSE)
print(summary(dat.pca))
print(dat.pca)
# Return output to console
sink()
# Plot PCA
jpeg(paste(resultPath,pcaTitle,".jpg", sep = ""))
par(xaxt = 'n')
plot(dat.pca, type = "l", main = pcaTitle)
dev.off()
}
# Function that prepares the data to put into the neural network. We bind the
# data leaving the target variable in the right most column. Every column
# corresponds to a different feature. Features are normalized using z-score
#
# Input:
#       dat = data set, last column must be th target variable
#       sizeTrainging = The percent of the data we want to train on
#       datanames = array of names that correspond to feature names
#       shuffle = if TRUE shuffle rowise, if false leave in time order
#       normalization = n for none, z for z score, mm for min-max
prepdata <- function(dat,dataNames,normalization = "n",sizeTraining = 0.8,  shuffle = FALSE, split = FALSE) {
# Set column names
colnames(dat) <- dataNames
# Remove any row that has NA in it
dat <- dat[complete.cases(dat),]
if (normalization == "z") {
dat <- apply(dat, 2, function(x) {
((x - mean(x)) / sd(x) )
})
}
else if (normalization ==  "mm") {
dat <- apply(dat, 2, function(x) {
((x - min(x)) / (max(x) - min(x)) )
})
}
# Shuffle row-wise if need be
if (shuffle == TRUE){
dat <- dat[sample(nrow(dat)),]
}
# Divid training and testing set
if (split == TRUE) {
# Get the number of data points
num_data <- dim(dat)[1]
# Find training and testing size
training_size <- num_data * sizeTraining
testing_size <- num_data * (1 - sizeTraining)
train_input <- dat[1:training_size,]
test_input <- dat[(training_size + 1):(training_size + testing_size),]
return (list(train_input, test_input))
} else {
return (dat)
}
}
# Trains neural network, runs prediction and creates visualizations
# Input:
#       trainInput = training input
#       shuffle = if TRUE change title to reflect the fact data is shuffled
runmodel <- function (trainInput, testInput,  accuracyTitle, residualTitle, resultPath) {
model_nnet <- nnet(x=as.data.frame(trainInput[,1:ncol(trainInput) - 1]), hidden = 10,
y=as.data.frame(trainInput[,ncol(trainInput)]), size=20,maxit = 10000, linout = TRUE,
trace = TRUE, decay=1e-3)
# Run Prediction
predicted <- predict(model_nnet, testInput, type = "raw")
# Compute Accuracy
accuracy <- (((predicted-testInput)/testInput)*100)[,ncol(trainInput)]
# Find the range we want to limit the plot to
bounds <- max(abs(ceiling(max(accuracy))),abs(floor(min(accuracy))))
# Create name of the output file
accuracy_file_name <- paste(resultPath,accuracyTitle, '.jpg', sep = "")
# Plot and save accuracy
jpeg (accuracy_file_name)
plot(accuracy, ylim = c(-bounds,bounds), ylab = "Percent",
xlab = "Year and Month (10/2014 - 09/2015)")
title (main = accuracyTitle)
dev.off()
accuracy_file_name <- paste(resultPath,accuracyTitle,'-100','.jpg', sep = "")
jpeg (accuracy_file_name)
plot(accuracy, ylim = c(-100,100), ylab = "Percent",
xlab = "Year and Month (10/2014 - 09/2015)")
title (main = accuracyTitle)
dev.off()
}
getpca(prepdata(housing_dat,names, normalization = "mm"), result_path, "PCA-Monroe-County-1", names)
source('~/Documents/MarketTrendPrediction/src/Model-Functions.r')
# file: Model-Functions.r
# Contributers: William Hammond
#               Harry Longwell
# Created: 11/19/2015
# Modified: 11/22/2015
library(nnet)
# Function that computes and plots PCA for a given data set.
# Input:
#       dat = data set
#       resultPath = Path to results folder
#       pcaTitle = title to plot
getpca <- function (dat, resultPath, pcaTitle,names) {
# Principle Component Analysis
# Log transform
#log.dat <- log(dat)
# Apply PCA
dat.pca <- prcomp(dat,
center = TRUE,
scale. = TRUE)
# This will pipe out put to file
sink (paste(resultPath,pcaTitle, '.txt', sep = ''), append = TRUE, split = FALSE)
print(summary(dat.pca))
print(dat.pca)
# Return output to console
sink()
# Plot PCA
jpeg(paste(resultPath,pcaTitle,".jpg", sep = ""))
plot(dat.pca, type = "l", main = pcaTitle)
dev.off()
}
# Function that prepares the data to put into the neural network. We bind the
# data leaving the target variable in the right most column. Every column
# corresponds to a different feature. Features are normalized using z-score
#
# Input:
#       dat = data set, last column must be th target variable
#       sizeTrainging = The percent of the data we want to train on
#       datanames = array of names that correspond to feature names
#       shuffle = if TRUE shuffle rowise, if false leave in time order
#       normalization = n for none, z for z score, mm for min-max
prepdata <- function(dat,dataNames,normalization = "n",sizeTraining = 0.8,  shuffle = FALSE, split = FALSE) {
# Set column names
colnames(dat) <- dataNames
# Remove any row that has NA in it
dat <- dat[complete.cases(dat),]
if (normalization == "z") {
dat <- apply(dat, 2, function(x) {
((x - mean(x)) / sd(x) )
})
}
else if (normalization ==  "mm") {
dat <- apply(dat, 2, function(x) {
((x - min(x)) / (max(x) - min(x)) )
})
}
# Shuffle row-wise if need be
if (shuffle == TRUE){
dat <- dat[sample(nrow(dat)),]
}
# Divid training and testing set
if (split == TRUE) {
# Get the number of data points
num_data <- dim(dat)[1]
# Find training and testing size
training_size <- num_data * sizeTraining
testing_size <- num_data * (1 - sizeTraining)
train_input <- dat[1:training_size,]
test_input <- dat[(training_size + 1):(training_size + testing_size),]
return (list(train_input, test_input))
} else {
return (dat)
}
}
# Trains neural network, runs prediction and creates visualizations
# Input:
#       trainInput = training input
#       shuffle = if TRUE change title to reflect the fact data is shuffled
runmodel <- function (trainInput, testInput,  accuracyTitle, residualTitle, resultPath) {
model_nnet <- nnet(x=as.data.frame(trainInput[,1:ncol(trainInput) - 1]), hidden = 10,
y=as.data.frame(trainInput[,ncol(trainInput)]), size=20,maxit = 10000, linout = TRUE,
trace = TRUE, decay=1e-3)
# Run Prediction
predicted <- predict(model_nnet, testInput, type = "raw")
# Compute Accuracy
accuracy <- (((predicted-testInput)/testInput)*100)[,ncol(trainInput)]
# Find the range we want to limit the plot to
bounds <- max(abs(ceiling(max(accuracy))),abs(floor(min(accuracy))))
# Create name of the output file
accuracy_file_name <- paste(resultPath,accuracyTitle, '.jpg', sep = "")
# Plot and save accuracy
jpeg (accuracy_file_name)
plot(accuracy, ylim = c(-bounds,bounds), ylab = "Percent",
xlab = "Year and Month (10/2014 - 09/2015)")
title (main = accuracyTitle)
dev.off()
accuracy_file_name <- paste(resultPath,accuracyTitle,'-100','.jpg', sep = "")
jpeg (accuracy_file_name)
plot(accuracy, ylim = c(-100,100), ylab = "Percent",
xlab = "Year and Month (10/2014 - 09/2015)")
title (main = accuracyTitle)
dev.off()
}
getpca(prepdata(housing_dat,names, normalization = "mm"), result_path, "PCA-Monroe-County-1", names)
# File: Monroe-Model.r
# Contributers: William Hammond
#               Harry Longwell
# Created:  11/22/2015
# Modified: 11/22/2015
#
# Description: This program loads in zillow data for monroe county and creates
#              a neural network used to predict median market value based
#              off of measures of market health.
source('Model-Functions.r')
data_path <- '../data/county/'
result_path <- '../results/Monroe/'
## Load in response variables
sold_for_gain <- read.csv( file = paste(data_path, 'County_PctOfHomesSellingForGain_AllHomes.csv', sep = ''))
sold_for_gain <- sold_for_gain[sold_for_gain$Metro == 'Rochester',]
sold_for_gain <- t(subset(sold_for_gain, select = X2010.10:X2015.09))
#sold_for_gain <- as.data.frame(approx(sold_for_gain, n = 500)$y)
increasing_in_value <- read.csv ( file = paste(data_path, 'County_PctOfHomesIncreasingInValues_AllHomes.csv', sep =''))
increasing_in_value <- increasing_in_value[increasing_in_value$Metro == 'Rochester',]
increasing_in_value <- increasing_in_value[increasing_in_value$RegionName == 'Monroe',]
increasing_in_value <- t(subset(increasing_in_value, select = X2010.10:X2015.09))
#increasing_in_value <-as.data.frame(approx(increasing_in_value, n = 500)$y)
pct_price_reduced <-read.csv ( file = paste(data_path, 'County_PctOfListingsWithPriceReductions_AllHomes.csv', sep = ''))
pct_price_reduced <- pct_price_reduced[pct_price_reduced$RegionName == 'Monroe',]
pct_price_reduced <- pct_price_reduced[pct_price_reduced$State == 'NY',]
pct_price_reduced <- t(subset(pct_price_reduced, select = X2010.10:X2015.09))
#pct_price_reduced <- as.data.frame(approx(pct_price_reduced, n = 500)$y)
median_pct_price_reduced <-read.csv ( file = paste(data_path, 'County_MedianPctOfPriceReduction_AllHomes.csv', sep = ''))
median_pct_price_reduced <- median_pct_price_reduced[median_pct_price_reduced$RegionName == 'Monroe',]
median_pct_price_reduced <- median_pct_price_reduced[median_pct_price_reduced$State == 'NY',]
median_pct_price_reduced <- t(subset(median_pct_price_reduced, select = X2010.10:X2015.09))
#median_pct_price_reduced <- as.data.frame(approx(median_pct_price_reduced, n = 500)$y)
ratio_foreclose <- read.csv ( file = paste(data_path, 'County_HomesSoldAsForeclosures-Ratio_AllHomes.csv', sep = ''))
ratio_foreclose <- ratio_foreclose [ratio_foreclose$RegionName == 'Monroe',]
ratio_foreclose <- ratio_foreclose [ratio_foreclose$Metro == 'Rochester',]
ratio_foreclose <- t(subset(ratio_foreclose, select = X2010.10:X2015.09))
#ratio_foreclose <- as.data.frame(approx(ratio_foreclose, n = 500)$y)
inventory_measure <- read.csv ( file = paste(data_path, 'InventoryMeasure_County_Public.csv', sep = ''))
inventory_measure <- inventory_measure [inventory_measure$CountyName == 'Monroe',]
inventory_measure <- inventory_measure [inventory_measure$Metro == 'Rochester',]
inventory_measure <- t(subset(inventory_measure, select = X2010.10:X2015.09))
#inventory_measure <- as.data.frame(approx(inventory_measure, n = 500)$y)
price_to_rent = read.csv(file = paste(data_path, 'County_PriceToRentRatio_AllHomes.csv', sep = ''))
price_to_rent <- price_to_rent[price_to_rent$RegionName == 'Monroe',]
price_to_rent <- price_to_rent[price_to_rent$State == 'NY',]
price_to_rent <- t(subset(price_to_rent, select = X2010.10:X2015.09))
#price_to_rent <- as.data.frame(approx(price_to_rent, n = 500)$y)
# Target Variable
median_sold_price <- read.csv( file = paste(data_path, 'County_MedianSoldPrice_AllHomes.csv', sep = ''))
median_sold_price <- median_sold_price[median_sold_price$Metro == 'Rochester',]
median_sold_price <- median_sold_price[median_sold_price$RegionName == 'Monroe',]
median_sold_price <- t(subset(median_sold_price, select = X2010.10:X2015.09))
#median_sold_price <- as.data.frame(approx(median_sold_price, n = 500)$y)
# Create array of columns names used for dataframe
names <- c('price_to_rent','sold_for_gain',
'increasing_in_value', 'pct_price_reduced','median_sold_price' )
# Bind all of the features into a dataframe
housing_dat = as.data.frame(cbind(price_to_rent,
sold_for_gain,increasing_in_value,pct_price_reduced,median_sold_price))
# Run PCA
getpca(prepdata(housing_dat,names, normalization = "mm"), result_path, "PCA-Monroe-County-1-No-Ratio-or-Inventory", names)
# Prepare training and testing data
prepared_data <- prepdata(housing_dat, names, normalization = "z", split = TRUE)
training_input <- as.data.frame(prepared_data[1])
testing_input <- as.data.frame(prepared_data[2])
# Run the model and save the results
runmodel(training_input, testing_input, "Monroe-Future-Accuracy-No-Ratio-or-Inventory", "test", result_path)
price_to_rent <- read.csv(file = paste(data_path, 'County_Turnover_AllHomes.csv', sep = ''))
price_to_rent
turnover <- read.csv(file = paste(data_path, 'County_Turnover_AllHomes.csv', sep = ''))
head(turnover)
turnover <- turnover[turnover$State == 'NY',]
turnover
head(turnover)
turnover <- read.csv(file = paste(data_path, 'County_Turnover_AllHomes.csv', sep = ''))
turnover <- turnover[turnover$RegionName == 'Monroe',]
turnover <- turnover[turnover$State == 'NY',]
turnover
turnover <- read.csv(file = paste(data_path, 'County_Turnover_AllHomes.csv', sep = ''))
turnover <- turnover[turnover$RegionName == 'Monroe',]
turnover <- turnover[turnover$State == 'NY',]
turnover <- t(subset(turnover, select = X2010.10:X2015.09))
turnover
sales <- read.csv(file = paste(data_path, 'Sales_County.csv', sep = ''))
sales <- sales[sales$RegionName == 'Monroe',]
sales <- sales[sales$State == 'NY',]
sales <- t(subset(sales, select = X2010.10:X2015.09))
sales <- read.csv(file = paste(data_path, 'Sales_County.csv', sep = ''))
sales <- sales[sales$RegionName == 'Monroe',]
sales <- sales[sales$State == 'NY',]
sales
sales <- read.csv(file = paste(data_path, 'Sales_County.csv', sep = ''))
#sales <- sales[sales$RegionName == 'Monroe',]
sales <- sales[sales$State == 'NY',]
sales
sales <- read.csv(file = paste(data_path, 'Sales_County.csv', sep = ''))
sales
sales$RegionName
sales$RegionName['Monroe']
sales$State
[sales$State == 'New York',]
sales <- sales[sales$State == 'New York',]
sales
sales <- sales[sales$RegionName == 'Monroe',]
sales
sales <- read.csv(file = paste(data_path, 'Sales_County.csv', sep = ''))
sales <- sales[sales$State == 'New York',]
sales$RegionName
print(sales$RegionName)
sales
sales[sales$RegionName == 'Monroe']
sales[sales$RegionName == 'New York']
sales$Apr.2012
sales <- read.csv(file = paste(data_path, 'Sales_County.csv', sep = ''))
sales[sales$RegionName == 'New York']
sales <- read.csv(file = paste(data_path, 'Sales_County.csv', sep = ''))
sales[sales$RegionName == 'New York']
sales
sales <- sales[sales$State == 'New York',]
sales
t(sales$RegionName)
# File: Monroe-Model.r
# Contributers: William Hammond
#               Harry Longwell
# Created:  11/22/2015
# Modified: 11/22/2015
#
# Description: This program loads in zillow data for monroe county and creates
#              a neural network used to predict median market value based
#              off of measures of market health.
source('Model-Functions.r')
data_path <- '../data/county/'
result_path <- '../results/Monroe/'
## Load in response variables
sold_for_gain <- read.csv( file = paste(data_path, 'County_PctOfHomesSellingForGain_AllHomes.csv', sep = ''))
sold_for_gain <- sold_for_gain[sold_for_gain$Metro == 'Rochester',]
sold_for_gain <- t(subset(sold_for_gain, select = X2010.10:X2015.09))
#sold_for_gain <- as.data.frame(approx(sold_for_gain, n = 500)$y)
increasing_in_value <- read.csv ( file = paste(data_path, 'County_PctOfHomesIncreasingInValues_AllHomes.csv', sep =''))
increasing_in_value <- increasing_in_value[increasing_in_value$Metro == 'Rochester',]
increasing_in_value <- increasing_in_value[increasing_in_value$RegionName == 'Monroe',]
increasing_in_value <- t(subset(increasing_in_value, select = X2010.10:X2015.09))
#increasing_in_value <-as.data.frame(approx(increasing_in_value, n = 500)$y)
pct_price_reduced <-read.csv ( file = paste(data_path, 'County_PctOfListingsWithPriceReductions_AllHomes.csv', sep = ''))
pct_price_reduced <- pct_price_reduced[pct_price_reduced$RegionName == 'Monroe',]
pct_price_reduced <- pct_price_reduced[pct_price_reduced$State == 'NY',]
pct_price_reduced <- t(subset(pct_price_reduced, select = X2010.10:X2015.09))
#pct_price_reduced <- as.data.frame(approx(pct_price_reduced, n = 500)$y)
median_pct_price_reduced <-read.csv ( file = paste(data_path, 'County_MedianPctOfPriceReduction_AllHomes.csv', sep = ''))
median_pct_price_reduced <- median_pct_price_reduced[median_pct_price_reduced$RegionName == 'Monroe',]
median_pct_price_reduced <- median_pct_price_reduced[median_pct_price_reduced$State == 'NY',]
median_pct_price_reduced <- t(subset(median_pct_price_reduced, select = X2010.10:X2015.09))
#median_pct_price_reduced <- as.data.frame(approx(median_pct_price_reduced, n = 500)$y)
ratio_foreclose <- read.csv ( file = paste(data_path, 'County_HomesSoldAsForeclosures-Ratio_AllHomes.csv', sep = ''))
ratio_foreclose <- ratio_foreclose [ratio_foreclose$RegionName == 'Monroe',]
ratio_foreclose <- ratio_foreclose [ratio_foreclose$Metro == 'Rochester',]
ratio_foreclose <- t(subset(ratio_foreclose, select = X2010.10:X2015.09))
#ratio_foreclose <- as.data.frame(approx(ratio_foreclose, n = 500)$y)
inventory_measure <- read.csv ( file = paste(data_path, 'InventoryMeasure_County_Public.csv', sep = ''))
inventory_measure <- inventory_measure [inventory_measure$CountyName == 'Monroe',]
inventory_measure <- inventory_measure [inventory_measure$Metro == 'Rochester',]
inventory_measure <- t(subset(inventory_measure, select = X2010.10:X2015.09))
#inventory_measure <- as.data.frame(approx(inventory_measure, n = 500)$y)
price_to_rent <- read.csv(file = paste(data_path, 'County_PriceToRentRatio_AllHomes.csv', sep = ''))
price_to_rent <- price_to_rent[price_to_rent$RegionName == 'Monroe',]
price_to_rent <- price_to_rent[price_to_rent$State == 'NY',]
price_to_rent <- t(subset(price_to_rent, select = X2010.10:X2015.09))
#price_to_rent <- as.data.frame(approx(price_to_rent, n = 500)$y)
turnover <- read.csv(file = paste(data_path, 'County_Turnover_AllHomes.csv', sep = ''))
turnover <- turnover[turnover$RegionName == 'Monroe',]
turnover <- turnover[turnover$State == 'NY',]
turnover <- t(subset(turnover, select = X2010.10:X2015.09))
# sales <- read.csv(file = paste(data_path, 'Sales_County.csv', sep = ''))
# sales <- sales[sales$RegionName == 'Monroe',]
# sales <- sales[sales$State == 'New York',]
# sales <- t(subset(sales, select = X2010.10:X2015.09))
# Target Variable
median_sold_price <- read.csv( file = paste(data_path, 'County_MedianSoldPrice_AllHomes.csv', sep = ''))
median_sold_price <- median_sold_price[median_sold_price$Metro == 'Rochester',]
median_sold_price <- median_sold_price[median_sold_price$RegionName == 'Monroe',]
median_sold_price <- t(subset(median_sold_price, select = X2010.10:X2015.09))
#median_sold_price <- as.data.frame(approx(median_sold_price, n = 500)$y)
# Create array of columns names used for dataframe
names <- c('price_to_rent','turnover','sold_for_gain',
'increasing_in_value', 'pct_price_reduced','median_sold_price' )
# Bind all of the features into a dataframe
housing_dat = as.data.frame(cbind(price_to_rent, turnover,
sold_for_gain,increasing_in_value,pct_price_reduced,median_sold_price))
# Run PCA
getpca(prepdata(housing_dat,names, normalization = "mm"), result_path, "PCA-Monroe-County-1-turnover", names)
# Prepare training and testing data
prepared_data <- prepdata(housing_dat, names, normalization = "z", split = TRUE)
training_input <- as.data.frame(prepared_data[1])
testing_input <- as.data.frame(prepared_data[2])
# Run the model and save the results
runmodel(training_input, testing_input, "Monroe-Future-Accuracy-turnover", "test", result_path)
approx(median_sold_price)$y
as.data.frame(approx(median_sold_price)$y)
plot(as.data.frame(approx(median_sold_price)$y))
plot(as.data.frame(approx(median_sold_price)$x))
plot(approx(median_sold_price))
approx(median_sold_price)$x
